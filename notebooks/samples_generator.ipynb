{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "1. Fetching the GPT-wiki-intro dataset\n",
    "2. Extracting first 1000 promts\n",
    "3. Creating dataset with 1000 promts, ids and generated text. Generated text is empty for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"aadityaubhat/GPT-wiki-intro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 63064638,\n",
       " 'url': 'https://en.wikipedia.org/wiki/Sexhow%20railway%20station',\n",
       " 'title': 'Sexhow railway station',\n",
       " 'wiki_intro': \"Sexhow railway station was a railway station built to serve the hamlet of Sexhow in North Yorkshire, England. The station was on the North Yorkshire and Cleveland's railway line between  and , which opened in 1857. The line was extended progressively until it met the Whitby & Pickering Railway at . Sexhow station was closed in 1954 to passengers and four years later to goods. The station was located  south of Stockton, and  west of Battersby railway station. History\\nThe station was opened in April 1857, when the line from Picton was opened up as far as . Mapping shows the station to have had three sidings in the goods yard, coal drops and a crane. The main station buildings were on the westbound (Picton direction) side of the station. The station was south of the village that it served, and was actually in the parish of Carlton in Cleveland, which has led to speculation that it was named Sexhow to avoid confusion with  railway station, which was originally named Carlton.\",\n",
       " 'generated_intro': 'Sexhow railway station was a railway station located in the town of Sexhow, on the Cumbrian Coast Line in North West England. The station was opened by the Lancashire and Yorkshire Railway on 7 October 1870. It was closed to passengers on 5 January 1950, and to goods on 12 May 1965. \\n\\nThe station building is now a private residence. There is a small amount of trackage remaining near the building, used currently by a local agricultural business.',\n",
       " 'title_len': 3,\n",
       " 'wiki_intro_len': 174,\n",
       " 'generated_intro_len': 78,\n",
       " 'prompt': \"200 word wikipedia style introduction on 'Sexhow railway station'\\n    Sexhow railway station was a railway station\",\n",
       " 'generated_text': ' located in the town of Sexhow, on the Cumbrian Coast Line in North West England. The station was opened by the Lancashire and Yorkshire Railway on 7 October 1870. It was closed to passengers on 5 January 1950, and to goods on 12 May 1965. \\n\\nThe station building is now a private residence. There is a small amount of trackage remaining near the building, used currently by a local agricultural business.',\n",
       " 'prompt_tokens': 25,\n",
       " 'generated_text_tokens': 88}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = base_df.drop(base_df[base_df.index >= 1000].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = base_df[['id', 'prompt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df['generated'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.to_csv('base.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facebook/opt-1.3b\n",
    "# facebook/opt-2.7b\n",
    "# facebook/opt-125m\n",
    "# meta-llama/Llama-2-7b-chat-hf\n",
    "# meta-llama/Llama-2-13b-chat-hf\n",
    "MODEL_PATH = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing quantization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # load in 4bit\n",
    "    bnb_4bit_quant_type=\"nf4\",  # use 4-bit NormalFloat quant type\n",
    "    bnb_4bit_use_double_quant=True, # use 4-bit NormalFloat quant type\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # use type with higher precision for computations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    # use_flash_attention_2=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH, trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load base dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.read_csv('base.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "- `generate_text` - generates text for given prompt\n",
    "- `extract_response` - extracts response from generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(promt: str):\n",
    "    input_ids = tokenizer.encode(promt, return_tensors=\"pt\")\n",
    "    generated_ids = base_model.generate(\n",
    "        input_ids,\n",
    "        max_length=500,\n",
    "        num_return_sequences=1,\n",
    "        # next time probably\n",
    "        # do_sample=True,\n",
    "        # top_k=50, \n",
    "        # top_p=0.95, \n",
    "    )\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response(text: str):\n",
    "    return text.split('\\n    ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "errors = []\n",
    "for index, row in tqdm(iterable = base_df.iterrows(), desc=f\"Processing model {MODEL_PATH}\", total=base_df.shape[0]):\n",
    "    try:\n",
    "        prompt = row['prompt']\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = base_model.generate(inputs, max_length=500)\n",
    "        generated_text = generate_text(prompt)\n",
    "        generated_text = extract_response(generated_text)\n",
    "        base_df.loc[index, 'generated'] = generated_text\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.to_csv('Llama-2-13b-chat-hf.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
