# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.15.2
#   kernelspec:
#     display_name: venv
#     language: python
#     name: python3
# ---

# +
# this notebook is used to process some public datasets and prepare for further usage

import os
from enum import Enum

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
# -

# # Read datasets

# ### GPT-wiki-intro
#
# https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro
#
# Dataset for training models to classify human written vs GPT/ChatGPT generated text. This dataset contains Wikipedia introductions and GPT (Curie) generated introductions for 150k topics.

df_wiki = pd.read_csv('GPT-wiki-intro.csv')

df_wiki.head()

df_wiki.shape

# ### DeepfakeTextDetect
#
# https://github.com/yafuly/DeepfakeTextDetect
#
# The dataset consists of 447,674 human-written and machine-generated texts from a wide range of sources in the wild:
#
# Human-written texts from 10 datasets covering a wide range of writing tasks, e.g., news article writing, story generation, scientific writing, etc.
# Machine-generated texts generated by 27 mainstream LLMs from 7 sources, e.g., OpenAI, LLaMA, and EleutherAI, etc.
# 6 systematic testbeds with increasing wildness and detection difficulty.
# 2 wilder test sets: (1) texts collected from new datasets and generated by GPT-4; (2) paraphrased texts.

df_deepfake = pd.concat([
    pd.read_csv('deepfakedetect/train.csv'),
    pd.read_csv('deepfakedetect/valid.csv'),
    pd.read_csv('deepfakedetect/test.csv'),
], ignore_index=True)

df_deepfake.head()

df_deepfake['label'].value_counts()

# ### TuringBench
#
# https://huggingface.co/datasets/turingbench/TuringBench/tree/main
#
# 168,612 articles from 19 machine text-generators and 1 human

# +
temp = []
for folder in os.listdir('./TuringBench'):
    for fname in ['test.csv', 'train.csv', 'valid.csv']:
        temp.append(pd.read_csv(f'TuringBench/{folder}/{fname}'))

df_turing = pd.concat(temp, ignore_index=True)
# -

df_turing.head()

df_turing.drop_duplicates(inplace=True)
df_turing['label'].value_counts()


# # Combine collect_data

class DataSource(str, Enum):
    """Enum with source datasets used."""

    deepfake = "DeepfakeTextDetect"
    turing = "TuringBench"
    wiki = "GPT-wiki-intro"


df_wiki.loc[0]['wiki_intro']

df_wiki.loc[0]['generated_text']

df_wiki.loc[0]['generated_intro']

df_wiki[df_wiki.apply(lambda x: x['generated_text'] not in x['generated_intro'], axis=1)]

human = df_wiki[['wiki_intro']].rename(columns={'wiki_intro': 'text'})
human['generator'] = None
human['is_generated'] = 0
machine = df_wiki[['generated_intro']].rename(columns={'generated_intro': 'text'})
machine['generator'] = 'gpt4'
machine['is_generated'] = 1
df_wiki = pd.concat([human, machine], ignore_index=True)
df_wiki['source'] = DataSource.wiki.value

df_wiki

df_deepfake.head()

df_deepfake['label'].value_counts()

assert list(df_deepfake['src'].apply(lambda x: 'human' in x)) == list(df_deepfake['label'].apply(lambda x: x == 1))

df_deepfake[df_deepfake['label'] == 0]['src'].value_counts().head(10)

# trubo ))))

df_deepfake['label'] = (~df_deepfake['label'].astype(bool)).astype(int)
df_deepfake.rename(columns={'src': 'generator', 'label': 'is_generated'}, inplace=True)
df_deepfake['generator'] = 'various'  # TODO: extract model name
df_deepfake.loc[df_deepfake['is_generated'] == 0, 'generator'] = None
df_deepfake['source'] = DataSource.deepfake.value
df_deepfake

df_deepfake['is_generated'].value_counts()

df_turing.rename(columns={'label': 'generator', 'Generation': 'text'}, inplace=True)
df_turing['is_generated'] = df_turing['generator'].apply(lambda x: x != 'human').astype(int)
df_turing.loc[df_turing['is_generated'] == 0, 'generator'] = None
df_turing['source'] = DataSource.turing.value
df_turing

data = pd.concat([df_wiki, df_deepfake, df_turing], ignore_index=True)
data

data['is_generated'].value_counts()

data.to_parquet('mgt_dataset.parquet')

# # Small EDA

data['tokens'] = data['text'].apply(word_tokenize)

data['len_tokens'] = data['tokens'].apply(len)

data.head()

data.to_parquet('mgt_dataset_tokenized.parquet')

fig = plt.figure(figsize=(12, 6), dpi=100)
sns.histplot(data=data, x='len_tokens', hue='is_generated', bins=100, multiple='stack', log_scale=True).set(title='Documents length distribution', xlabel='Tokens count');


